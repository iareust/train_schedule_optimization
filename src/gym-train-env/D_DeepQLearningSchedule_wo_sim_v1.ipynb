{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\nwindow.scroll_flag = true\nwindow.scroll_exit = false\nwindow.scroll_delay = 100\n\n$(\".output_scroll\").each(function() {\n    $(this)[0].scrollTop = $(this)[0].scrollHeight;\n});\n\nfunction callScrollToBottom() {\n    setTimeout(scrollToBottom, window.scroll_delay);\n}\n\nfunction scrollToBottom() {\n    if (window.scroll_exit) {\n        return;\n    }\n    if (!window.scroll_flag) {\n        callScrollToBottom();\n        return;\n    };\n\n    $(\".output_scroll\").each(function() {\n        if (!$(this).attr('scroll_checkbox')){\n            window.scroll_flag = true;\n            $(this).attr('scroll_checkbox',true);\n            var div = document.createElement('div');\n            var checkbox = document.createElement('input');\n            checkbox.type = \"checkbox\";\n            checkbox.onclick = function(){window.scroll_flag = checkbox.checked}\n            checkbox.checked = \"checked\"\n            div.append(\"Auto-Scroll-To-Bottom: \");\n            div.append(checkbox);\n            $(this).parent().before(div);\n        }\n\n        $(this)[0].scrollTop = $(this)[0].scrollHeight;\n    });\n    callScrollToBottom();\n}\nscrollToBottom();\n\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "window.scroll_flag = true\n",
    "window.scroll_exit = false\n",
    "window.scroll_delay = 100\n",
    "\n",
    "$(\".output_scroll\").each(function() {\n",
    "    $(this)[0].scrollTop = $(this)[0].scrollHeight;\n",
    "});\n",
    "\n",
    "function callScrollToBottom() {\n",
    "    setTimeout(scrollToBottom, window.scroll_delay);\n",
    "}\n",
    "\n",
    "function scrollToBottom() {\n",
    "    if (window.scroll_exit) {\n",
    "        return;\n",
    "    }\n",
    "    if (!window.scroll_flag) {\n",
    "        callScrollToBottom();\n",
    "        return;\n",
    "    };\n",
    "\n",
    "    $(\".output_scroll\").each(function() {\n",
    "        if (!$(this).attr('scroll_checkbox')){\n",
    "            window.scroll_flag = true;\n",
    "            $(this).attr('scroll_checkbox',true);\n",
    "            var div = document.createElement('div');\n",
    "            var checkbox = document.createElement('input');\n",
    "            checkbox.type = \"checkbox\";\n",
    "            checkbox.onclick = function(){window.scroll_flag = checkbox.checked}\n",
    "            checkbox.checked = \"checked\"\n",
    "            div.append(\"Auto-Scroll-To-Bottom: \");\n",
    "            div.append(checkbox);\n",
    "            $(this).parent().before(div);\n",
    "        }\n",
    "\n",
    "        $(this)[0].scrollTop = $(this)[0].scrollHeight;\n",
    "    });\n",
    "    callScrollToBottom();\n",
    "}\n",
    "scrollToBottom();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.simulation.example_states import RL_Network\n",
    "import gym\n",
    "import gym_train_env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'2.1.0'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 100000 \n",
    "\n",
    "initial_collect_steps = 1000\n",
    "collect_steps_per_iteration = 1\n",
    "replay_buffer_max_length = 100000\n",
    "\n",
    "batch_size = 64 \n",
    "learning_rate = 1e-1\n",
    "log_interval = 200\n",
    "\n",
    "num_eval_episodes = 10 \n",
    "eval_interval = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'train-tf-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = suite_gym.load(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1))"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env.action_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layer_params = (200,)\n",
    "q_net = q_network.QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer QNetwork is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer TargetQNetwork is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(1)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "18.888887"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_avg_return(eval_env, agent.policy, num_eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy, buffer):\n",
    "  time_step = environment.current_time_step()\n",
    "  action_step = policy.action(time_step)\n",
    "  next_time_step = environment.step(action_step.action)\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "  # Add trajectory to the replay buffer\n",
    "  buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "  for _ in range(steps):\n",
    "    collect_step(env, policy, buffer)\n",
    "\n",
    "collect_data(train_env, random_policy, replay_buffer, steps=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 200: loss = 107.78077697753906\n",
      "step = 400: loss = 35.864044189453125\n",
      "step = 600: loss = 24.767192840576172\n",
      "step = 800: loss = 16.58395004272461\n",
      "step = 1000: loss = 7.2505083084106445\n",
      "step = 1000: Average Return = 6.0\n",
      " 8:32 -  8:32 -  0: 0 -  0: 0 -  0: 0 -  0: 0 - \n",
      " 0: 0 -  0: 0 -  0: 0 -  0: 0 -  0: 0 -  0: 0 - \n",
      " 0: 0 -  0: 0 -  0: 0 -  0: 0 -  0: 0 -  0: 0 - \n",
      "step = 1200: loss = 5.90922212600708\n",
      "step = 1400: loss = 26.132858276367188\n",
      "step = 1600: loss = 44.414642333984375\n",
      "step = 1800: loss = 49.58376693725586\n",
      "step = 2000: loss = 19.705150604248047\n",
      "step = 2000: Average Return = 45.0\n",
      " 0: 1 -  4:16 -  8:32 -  8:32 -  0: 0 -  0: 0 - \n",
      " 0: 0 -  0: 0 -  0: 0 -  0: 0 -  0: 0 -  0: 0 - \n",
      " 0: 0 -  0: 0 -  0: 0 -  0: 0 -  0: 0 -  0: 0 - \n",
      "step = 2200: loss = 56.35797119140625\n",
      "step = 2400: loss = 71.66030883789062\n",
      "step = 2600: loss = 18.237262725830078\n",
      "step = 2800: loss = 130.65721130371094\n",
      "step = 3000: loss = 32.78290557861328\n",
      "step = 3000: Average Return = 45.0\n",
      " 0: 1 -  4:16 -  8:32 -  8:32 -  0: 0 -  0: 0 - \n",
      " 0: 0 -  0: 0 -  0: 0 -  0: 0 -  0: 0 -  0: 0 - \n",
      " 0: 0 -  0: 0 -  0: 0 -  0: 0 -  0: 0 -  0: 0 - \n",
      "step = 3200: loss = 24.02081298828125\n",
      "step = 3400: loss = 27.956928253173828\n",
      "step = 3600: loss = 29.44459342956543\n",
      "step = 3800: loss = 48.36256408691406\n",
      "step = 4000: loss = 59.7111930847168\n",
      "step = 4000: Average Return = 6.0\n",
      " 8:32 -  8:32 -  0: 0 -  0: 0 -  0: 0 -  0: 0 - \n",
      " 0: 0 -  0: 0 -  0: 0 -  0: 0 -  0: 0 -  0: 0 - \n",
      " 0: 0 -  0: 0 -  0: 0 -  0: 0 -  0: 0 -  0: 0 - \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-22-5221d6496a3b>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     18\u001B[0m   \u001B[0;31m# Collect a few steps using collect_policy and save to the replay buffer.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m   \u001B[0;32mfor\u001B[0m \u001B[0m_\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcollect_steps_per_iteration\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 20\u001B[0;31m     \u001B[0mcollect_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_env\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0magent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect_policy\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreplay_buffer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     21\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m   \u001B[0;31m# Sample a batch of data from the buffer and update the agent's network.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-19-312f3017fc41>\u001B[0m in \u001B[0;36mcollect_step\u001B[0;34m(environment, policy, buffer)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mcollect_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0menvironment\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpolicy\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbuffer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m   \u001B[0mtime_step\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0menvironment\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcurrent_time_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m   \u001B[0maction_step\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpolicy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtime_step\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m   \u001B[0mnext_time_step\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0menvironment\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction_step\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m   \u001B[0mtraj\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrajectory\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_transition\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtime_step\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maction_step\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnext_time_step\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/AIForTrainScheduling/venv/lib/python3.7/site-packages/tf_agents/policies/tf_policy.py\u001B[0m in \u001B[0;36maction\u001B[0;34m(self, time_step, policy_state, seed)\u001B[0m\n\u001B[1;32m    277\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_automatic_state_reset\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    278\u001B[0m       \u001B[0mpolicy_state\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_maybe_reset_state\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtime_step\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpolicy_state\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 279\u001B[0;31m     \u001B[0mstep\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0maction_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtime_step\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtime_step\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpolicy_state\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpolicy_state\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mseed\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mseed\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    280\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    281\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mclip_action\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maction_spec\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/AIForTrainScheduling/venv/lib/python3.7/site-packages/tf_agents/utils/common.py\u001B[0m in \u001B[0;36mwith_check_resource_vars\u001B[0;34m(*fn_args, **fn_kwargs)\u001B[0m\n\u001B[1;32m    152\u001B[0m         \u001B[0;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    153\u001B[0m         \u001B[0;31m# autodep-like behavior is already expected of fn.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 154\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mfn_args\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mfn_kwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    155\u001B[0m       \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mresource_variables_enabled\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    156\u001B[0m         \u001B[0;32mraise\u001B[0m \u001B[0mRuntimeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mMISSING_RESOURCE_VARIABLES_ERROR\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/AIForTrainScheduling/venv/lib/python3.7/site-packages/tf_agents/policies/epsilon_greedy_policy.py\u001B[0m in \u001B[0;36m_action\u001B[0;34m(self, time_step, policy_state, seed)\u001B[0m\n\u001B[1;32m     90\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m_action\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime_step\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpolicy_state\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mseed\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     91\u001B[0m     \u001B[0mseed_stream\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtfp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mutil\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSeedStream\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mseed\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mseed\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msalt\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'epsilon_greedy'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 92\u001B[0;31m     \u001B[0mgreedy_action\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_greedy_policy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtime_step\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpolicy_state\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     93\u001B[0m     \u001B[0mrandom_action\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_random_policy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtime_step\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mseed_stream\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     94\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/AIForTrainScheduling/venv/lib/python3.7/site-packages/tf_agents/policies/tf_policy.py\u001B[0m in \u001B[0;36maction\u001B[0;34m(self, time_step, policy_state, seed)\u001B[0m\n\u001B[1;32m    277\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_automatic_state_reset\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    278\u001B[0m       \u001B[0mpolicy_state\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_maybe_reset_state\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtime_step\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpolicy_state\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 279\u001B[0;31m     \u001B[0mstep\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0maction_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtime_step\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtime_step\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpolicy_state\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpolicy_state\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mseed\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mseed\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    280\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    281\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mclip_action\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maction_spec\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/AIForTrainScheduling/venv/lib/python3.7/site-packages/tf_agents/utils/common.py\u001B[0m in \u001B[0;36mwith_check_resource_vars\u001B[0;34m(*fn_args, **fn_kwargs)\u001B[0m\n\u001B[1;32m    152\u001B[0m         \u001B[0;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    153\u001B[0m         \u001B[0;31m# autodep-like behavior is already expected of fn.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 154\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mfn_args\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mfn_kwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    155\u001B[0m       \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mresource_variables_enabled\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    156\u001B[0m         \u001B[0;32mraise\u001B[0m \u001B[0mRuntimeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mMISSING_RESOURCE_VARIABLES_ERROR\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/AIForTrainScheduling/venv/lib/python3.7/site-packages/tf_agents/policies/tf_policy.py\u001B[0m in \u001B[0;36m_action\u001B[0;34m(self, time_step, policy_state, seed)\u001B[0m\n\u001B[1;32m    470\u001B[0m     actions = tf.nest.map_structure(\n\u001B[1;32m    471\u001B[0m         \u001B[0;32mlambda\u001B[0m \u001B[0md\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mreparameterized_sampling\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msample\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0md\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mseed\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mseed_stream\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 472\u001B[0;31m         distribution_step.action)\n\u001B[0m\u001B[1;32m    473\u001B[0m     \u001B[0minfo\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdistribution_step\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    474\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0memit_log_probability\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/AIForTrainScheduling/venv/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py\u001B[0m in \u001B[0;36mmap_structure\u001B[0;34m(func, *structure, **kwargs)\u001B[0m\n\u001B[1;32m    566\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    567\u001B[0m   return pack_sequence_as(\n\u001B[0;32m--> 568\u001B[0;31m       \u001B[0mstructure\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mentries\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    569\u001B[0m       expand_composites=expand_composites)\n\u001B[1;32m    570\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/AIForTrainScheduling/venv/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    566\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    567\u001B[0m   return pack_sequence_as(\n\u001B[0;32m--> 568\u001B[0;31m       \u001B[0mstructure\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mentries\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    569\u001B[0m       expand_composites=expand_composites)\n\u001B[1;32m    570\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/AIForTrainScheduling/venv/lib/python3.7/site-packages/tf_agents/policies/tf_policy.py\u001B[0m in \u001B[0;36m<lambda>\u001B[0;34m(d)\u001B[0m\n\u001B[1;32m    469\u001B[0m     \u001B[0mdistribution_step\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_distribution\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtime_step\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpolicy_state\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    470\u001B[0m     actions = tf.nest.map_structure(\n\u001B[0;32m--> 471\u001B[0;31m         \u001B[0;32mlambda\u001B[0m \u001B[0md\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mreparameterized_sampling\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msample\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0md\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mseed\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mseed_stream\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    472\u001B[0m         distribution_step.action)\n\u001B[1;32m    473\u001B[0m     \u001B[0minfo\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdistribution_step\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/AIForTrainScheduling/venv/lib/python3.7/site-packages/tf_agents/distributions/reparameterized_sampling.py\u001B[0m in \u001B[0;36msample\u001B[0;34m(distribution, reparam, **kwargs)\u001B[0m\n\u001B[1;32m     49\u001B[0m       \u001B[0;32mreturn\u001B[0m \u001B[0mdistribution\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconvert_to_one_hot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msamples\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     50\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 51\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mdistribution\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msample\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/PycharmProjects/AIForTrainScheduling/venv/lib/python3.7/site-packages/tensorflow_probability/python/distributions/distribution.py\u001B[0m in \u001B[0;36msample\u001B[0;34m(self, sample_shape, seed, name, **kwargs)\u001B[0m\n\u001B[1;32m    856\u001B[0m       \u001B[0msamples\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0ma\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;31m`\u001B[0m \u001B[0;32mwith\u001B[0m \u001B[0mprepended\u001B[0m \u001B[0mdimensions\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0msample_shape\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    857\u001B[0m     \"\"\"\n\u001B[0;32m--> 858\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_sample_n\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msample_shape\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mseed\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    859\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    860\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m_call_log_prob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/AIForTrainScheduling/venv/lib/python3.7/site-packages/tensorflow_probability/python/distributions/distribution.py\u001B[0m in \u001B[0;36m_call_sample_n\u001B[0;34m(self, sample_shape, seed, name, **kwargs)\u001B[0m\n\u001B[1;32m    835\u001B[0m       samples = self._sample_n(\n\u001B[1;32m    836\u001B[0m           n, seed=seed() if callable(seed) else seed, **kwargs)\n\u001B[0;32m--> 837\u001B[0;31m       \u001B[0mbatch_event_shape\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msamples\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    838\u001B[0m       \u001B[0mfinal_shape\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconcat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0msample_shape\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_event_shape\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    839\u001B[0m       \u001B[0msamples\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msamples\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfinal_shape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/AIForTrainScheduling/venv/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001B[0m in \u001B[0;36mshape_v2\u001B[0;34m(input, out_type, name)\u001B[0m\n\u001B[1;32m    517\u001B[0m     \u001B[0mA\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;31m`\u001B[0m \u001B[0mof\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mout_type\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    518\u001B[0m   \"\"\"\n\u001B[0;32m--> 519\u001B[0;31m   \u001B[0;32mreturn\u001B[0m \u001B[0mshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mout_type\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    520\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    521\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/AIForTrainScheduling/venv/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001B[0m in \u001B[0;36mshape\u001B[0;34m(input, name, out_type)\u001B[0m\n\u001B[1;32m    543\u001B[0m     \u001B[0mA\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;31m`\u001B[0m \u001B[0mof\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mout_type\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    544\u001B[0m   \"\"\"\n\u001B[0;32m--> 545\u001B[0;31m   \u001B[0;32mreturn\u001B[0m \u001B[0mshape_internal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mout_type\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mout_type\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    546\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    547\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/AIForTrainScheduling/venv/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001B[0m in \u001B[0;36mshape_internal\u001B[0;34m(input, name, optimize, out_type)\u001B[0m\n\u001B[1;32m    571\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0moptimize\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0minput_shape\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_fully_defined\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    572\u001B[0m           \u001B[0;32mreturn\u001B[0m \u001B[0mconstant\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput_shape\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mas_list\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mout_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 573\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mgen_array_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mout_type\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mout_type\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    574\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    575\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/AIForTrainScheduling/venv/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\u001B[0m in \u001B[0;36mshape\u001B[0;34m(input, out_type, name)\u001B[0m\n\u001B[1;32m   8217\u001B[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001B[1;32m   8218\u001B[0m         \u001B[0m_ctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_context_handle\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtld\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdevice_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"Shape\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 8219\u001B[0;31m         tld.op_callbacks, input, \"out_type\", out_type)\n\u001B[0m\u001B[1;32m   8220\u001B[0m       \u001B[0;32mreturn\u001B[0m \u001B[0m_result\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   8221\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0m_core\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_FallbackException\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "  %%time\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  for _ in range(collect_steps_per_iteration):\n",
    "    collect_step(train_env, agent.collect_policy, replay_buffer)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "    eval_py_env.render()\n",
    "    returns.append(avg_return)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iterations = range(0, num_iterations+1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylim(top=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(20):\n",
    "    time_step = eval_env.reset()\n",
    "    while not time_step.is_last():\n",
    "        action_step = agent.policy.action(time_step)\n",
    "        time_step = eval_env.step(action_step.action)\n",
    "    \n",
    "    eval_py_env.render()\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}